input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => ["channel-svc.public.channels"]
    group_id => "logstash_debezium_consumer"
    auto_offset_reset => "earliest"
    codec => "json"
  }
}

filter {
  ruby {
    code => '
      p = event.get("payload")
      if p.nil?
        event.cancel
        return
      end

      op = p["op"]

      case op
      when "d"
        before = p["before"] || {}
        event.set("[@metadata][es_action]", "delete")
        event.set("[@metadata][doc_id]", before["id"])
        event.remove("schema")
        event.remove("payload")

      when "c", "u", "r"  # create / update / snapshot read
        after = p["after"] || {}
        event.set("[@metadata][es_action]", "index")
        event.set("[@metadata][doc_id]", after["id"])

        event.remove("schema")
        event.remove("payload")
        after.each { |k, v| event.set(k, v) }

      else
        # Unknown op -> drop to be safe
        event.cancel
      end
    '
  }

  if [created_at] {
    date { match => ["created_at", "ISO8601"] target => "created_at" }
  }
  if [updated_at] {
    date { match => ["updated_at", "ISO8601"] target => "updated_at" }
  }

  mutate {
    remove_field => ["@version", "host", "message", "@timestamp", "event"]
  }
}

output {
  elasticsearch {
    hosts             => ["http://es:9200"]
    index             => "channels"
    action            => "%{[@metadata][es_action]}"
    document_id       => "%{[@metadata][doc_id]}"
  }
}